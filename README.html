<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>JSTORr</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>





</head>

<body>
<h1>JSTORr</h1>

<p>Simple exploratory text mining of journal articles from JSTOR&#39;s Data for Research service.</p>

<h2>Objective</h2>

<p>The aim of this package is provide some simple functions in <code>R</code> to explore changes in word frequencies over time in a specific journal archive. Currently there are functions to explore changes in:</p>

<ul>
<li>a single word (ie. plot the relative frequency of a 1-gram over time)</li>
<li>two words independantly (ie. plot the relative frequency of two 1-grams over time)</li>
<li>sets of words (ie. plot the relative frequency of a single group of mulitple 1-grams over time)</li>
<li>correlations between two words over time (ie. plot the correlation of two 1-grams over time)</li>
<li>correlations between two sets of words over time (ie. plot the correlation two sets of multiple 1-grams over time)</li>
<li>all of the above with bigrams (a sequence of two words)</li>
<li>topic models with the <code>lda</code> package for full <code>R</code> solution or the MALLET Java-based program (if installing that is an option)</li>
<li>most frequent words by n-year ranges of documents (ie. top words in all documents published in 2-5-10 year ranges, whatever you like)</li>
<li>the top n words correlated a word by n-year ranges of documents (ie. the top 20 words associated with the word &#39;pirate&#39; in 5 year ranges)</li>
</ul>

<h2>How to install</h2>

<p>First, make sure you&#39;ve got Hadley Wickham&#39;s excellent devtools package installed. If you haven&#39;t got it, you can get it with these lines in your R console:</p>

<pre><code>install.packages(pkgs = &quot;devtools&quot;, dependencies = TRUE)
</code></pre>

<p>Then, use the <code>install_github()</code> function to fetch this package from github:</p>

<pre><code>library(devtools)
install_github(repo = &quot;JSTORr&quot;, username = &quot;UW-ARCHY-textual-macroanalysis-lab&quot;)
</code></pre>

<p>Error messages relating to rJava can probably be fixed by following exactly the instructions <a href="http://stackoverflow.com/a/7604469/1036500">here</a>.</p>

<h2>How to get started</h2>

<p>First, go to JSTOR&#39;s <a href="http://dfr.jstor.org/">Data for Research service</a> and make a request for data. The DfR service makes available large numbers of journal articles in a format that is convenient for text mining. When making a request for data to use with this package, you <strong>must</strong> chose:</p>

<ul>
<li><code>CSV</code> as the &#39;output format&#39;, not <code>XML</code>, which is the default</li>
<li><code>Word Counts</code> <strong>and</strong> <code>bigrams</code> as the &#39;Data Type&#39;</li>
</ul>

<p>Second, once you&#39;ve downloaded the zip file that is the &#39;full dataset&#39; from DfR then you can start <code>R</code>, install this package and run this function: </p>

<pre><code>unpack &lt;- JSTOR_unpack() # takes no arguments, but watch for prompts to enter details
</code></pre>

<p>Third, have fun exploring the other functions in the package!</p>

<h2>Typical workflow</h2>

<p>Here&#39;s one way to make use of this package:</p>

<p>First, go to <a href="http://dfr.jstor.org/">Data for Research service</a> and request data as specified above and download the zip file when it&#39;s available (it can take a few hours to days for DfR to prepare your archive). No need to unzip, that&#39;s done by the package.</p>

<p>Second, start <code>R</code> and run something like <code>unpack &lt;- JSTOR_unpack()</code> and paste in the directory and zip file name when prompted. Then you&#39;ll get a data object <code>unpack</code>, containing 1-grams, 2-grams and bibliographic data.</p>

<p>Third, explore some visualisations of key words over time with <code>JSTOR_1word</code>, <code>JSTOR_2words</code>, <code>JSTOR_1bigram</code>, <code>JSTOR_2bigrams</code>, and correlations of words over time with <code>JSTOR_2wordcor</code> and <code>JSTOR_2bigramscor</code></p>

<p>Fourth, put the documents into a corpus with <code>JSTOR_corpusofnouns</code> and explore further with more complex text analysis methods. The corpus can be changed to a Document Term Matrix using the <code>tm</code> package which has many advanced text mining methods. </p>

<p>Fifth, determine the most frequently used words at various intervals over time with <code>JSTOR_freqwords</code>. </p>

<p>Sixth, identify and visualise the words most strongly correlated with a word at various intervals over time with <code>JSTOR_findassocs</code></p>

<p>Seventh, generate topic models with <code>JSTOR_lda</code> (using the <code>lda</code> package, it&#39;s a lot faster than <code>topicmodels</code>) and <code>JSTOR_MALLET</code>. The latter function requires MALLET to be installed on your computer. See more about MALLET here <a href="http://mallet.cs.umass.edu/topics.php">http://mallet.cs.umass.edu/topics.php</a> and <a href="http://programminghistorian.org/lessons/topic-modeling-and-mallet">http://programminghistorian.org/lessons/topic-modeling-and-mallet</a> </p>

<p>Eighth, visualise and explore the output from the topic models. Identify the hot and cold topics in the corpus with <code>JSTOR_lda_hotncoldtopics</code> (if you generated the topic model with <code>JSTOR_lda</code>) or <code>JSTOR_MALLET_hotncoldtopics</code> (if you used <code>JSTOR_MALLET</code> to make the topic model)</p>

<h2>Limitations and Disclaimer</h2>

<p>Currently this package is intended for the exploration of a single journal archive. For example, all of the articles held by JSTOR of one journal or on one subject. It may be useful for other types of DfR archives, but has not yet been widely tested. Also, I am not a programmer, computer scientist, statistician, lawyer, etc. This is a work in progress and there is currently very little custom error handling (the more cryptic errors are usually due to a search for a word or bigram that does not exist in the archive). Use at your own risk, and fork and share as you like. </p>

<h2>Acknowledgements</h2>

<p>Many of the ideas for these functions have come directly from the prolific and creative research of Andrew Goldstone, Jonathan Goodwin, Shawn Graham, Matt Jockers, David Mimno, Ben Schmidt and Ted Underwood. None of them are responsible for the consequences of use of this package, no matter how awful, even if they arise from flaws in it (I take responsibility for the flaws). </p>

</body>

</html>

