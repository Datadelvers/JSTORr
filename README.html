<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>JSTORr</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>





</head>

<body>
<h1>JSTORr</h1>

<p>Simple exploratory text mining and document clustering of journal articles from JSTOR&#39;s Data for Research service.</p>

<h2>Objective</h2>

<p>The aim of this package is provide some simple functions in <code>R</code> to explore changes in word frequencies over time in a specific journal archive. Currently there are functions to explore changes in:</p>

<ul>
<li>a single word (ie. plot the relative frequency of a 1-gram over time)</li>
<li>two words independantly (ie. plot the relative frequency of two 1-grams over time)</li>
<li>sets of words (ie. plot the relative frequency of a single group of mulitple 1-grams over time)</li>
<li>correlations between two words over time (ie. plot the correlation of two 1-grams over time)</li>
<li>correlations between two sets of words over time (ie. plot the correlation two sets of multiple 1-grams over time)</li>
<li>all of the above with bigrams (a sequence of two words)</li>
<li>the most frequent words by n-year ranges of documents (ie. top words in all documents published in 2-5-10 year ranges, whatever you like)</li>
<li>the top n words correlated a word by n-year ranges of documents (ie. the top 20 words associated with the word &#39;pirate&#39; in 5 year ranges)</li>
<li>various methods (k-means, PCA, affinity propagation) to detect clusters in a set of documents containing a word or set of words</li>
<li>topic models with the <code>lda</code> package for full <code>R</code> solution or the Java-based MALLET program (if installing that is an option, currently implemented here for Windows only) </li>
</ul>

<h2>How to install</h2>

<p>First, make sure you&#39;ve got Hadley Wickham&#39;s excellent devtools package installed. If you haven&#39;t got it, you can get it with these lines in your R console:</p>

<pre><code>install.packages(pkgs = &quot;devtools&quot;, dependencies = TRUE)
</code></pre>

<p>Then, use the <code>install_github()</code> function to fetch this package from github:</p>

<pre><code>library(devtools)
install_github(repo = &quot;JSTORr&quot;, username = &quot;UW-ARCHY-textual-macroanalysis-lab&quot;)
</code></pre>

<p>Error messages relating to rJava can probably be fixed by following exactly the instructions <a href="http://stackoverflow.com/a/7604469/1036500">here</a>.</p>

<h2>How to get started</h2>

<p>First, go to JSTOR&#39;s <a href="http://dfr.jstor.org/">Data for Research service</a> and make a request for data. The DfR service makes available large numbers of journal articles in a format that is convenient for text mining. When making a request for data to use with this package, you <strong>must</strong> chose:</p>

<ul>
<li><code>CSV</code> as the &#39;output format&#39;, not <code>XML</code>, which is the default</li>
<li><code>Word Counts</code> <strong>and</strong> <code>bigrams</code> as the &#39;Data Type&#39;</li>
</ul>

<p>Second, once you&#39;ve downloaded and unzipped the zip file that is the &#39;full dataset&#39; from DfR then you can start <code>R</code> (it&#39;s highly recommended to use <a href="http://www.rstudio.com/ide/download/">RStudio</a> when working with this package, much easier to manage the plot output) and work through the steps in the next section.</p>

<h2>Typical workflow</h2>

<p>Here&#39;s how to make use of this package:</p>

<p>First, go to <a href="http://dfr.jstor.org/">Data for Research service</a> and request data as specified above and download the zip file when it&#39;s available (it can take a few hours to days for DfR to prepare your archive). Unzip the file and make a note of its location on your computer (in R, you can unzip like this: <code>unzip(&quot;2013.6.4.usytW8LZ.zip&quot;)</code> with your zip file name in between the quote marks).</p>

<p>Second, start <a href="http://www.rstudio.com/ide/download/">RStudio</a> and run: </p>

<pre><code># change the path to where you unzipped your file on your computer
unpack1grams &lt;- JSTOR_unpack1grams(path = &quot;C:/Users/marwick/Downloads/JSTOR&quot;)
</code></pre>

<p>but change the path value to suit your system and watch the console progress bars. Then you&#39;ll get a data object <code>unpack1grams</code>, containing a document term matrix of 1-grams and a data frame of bibliographic data.</p>

<p>Third, now you&#39;re ready to explore some visualisations of key words over time with <code>JSTOR_1word</code>, <code>JSTOR_2words</code>, and correlations of words over time with <code>JSTOR_2wordcor</code>, for example:</p>

<pre><code># one word over time
JSTOR_1word(unpack1grams, &quot;pirate&quot;)
# two words over time
JSTOR_2words(unpack1grams, &quot;pirate&quot;, &quot;navy&quot;)
# correlation of words over time
JSTOR_2wordcor(unpack1grams, &quot;pirate&quot;, &quot;navy&quot;)
</code></pre>

<p>Fourth, use <code>JSTOR_dtmofnouns</code> to create a document term matrix of nouns only, then investigate the most frequent words over time with <code>JSTOR_freqwords</code> and analyse correlations over time of all words with a word of interest with <code>JSTOR_findassocs</code>.  For example,</p>

<pre><code># subset the words to get nouns only
nouns &lt;-  JSTOR_dtmofnouns(unpack1grams, sparse = 0.75)
# plot and tabulate the most frequent words over time
JSTOR_freqwords(unpack1grams, nouns)
# plot and tabulate words correlated with &#39;pirate&#39; over time
JSTOR_findassocs(unpack1grams, nouns, &quot;pirate&quot;)
</code></pre>

<p>To optimise the output from these functions you must add words to the stopword list and then repeat these functions a few times over until the results look more relevant. Typically you&#39;ll need to add words like &#39;journal&#39;, &#39;press&#39;, &#39;research&#39; and so on to your stopword list. You can find the location of the stopword list on your computer by running this line <code>paste0(.libPaths()[1], &quot;/tm/stopwords/english.dat&quot;)</code> Then you can just edit the stopword list in RStudio or any text editor. Be careful not to have any errant spaces after a word on the list.</p>

<p>Fifth, explore document clusters using <code>JSTOR_clusterbywords</code> to calculate Affinity Propagation Clustering, K-means Clustering and Principal Components Analysis on a document term matrix. For example, </p>

<pre><code># plot and tabulate clusters of documents that contain the word &#39;pirate&#39;
JSTOR_clusterbywords(nouns, &#39;pirate&#39;)
</code></pre>

<p>Sixth, generate topic models with <code>JSTOR_lda</code> (using the <code>lda</code> package, it&#39;s a lot faster than <code>topicmodels</code>). Expore the model output with <code>JSTOR_lda_docdists</code> and <code>JSTOR_lda_topicdists</code>. Identify hot and cold topics in the corpus with <code>JSTOR_lda_hotncoldtopics</code>. Remember that editing the stopwords list may help to make the topics more distinctive. </p>

<pre><code># generate topic model with 50 topics (an arbitrary choice)
my_model &lt;- JSTOR_lda(unpack1grams, nouns, 50)
# visualise document distances by topics
JSTOR_lda_docdists(my_model)
# plot and tabulate hot and cold topics
JSTOR_lda_hotncoldtopics(my_model)
</code></pre>

<p>Or if you have MALLET installed, you can run <code>JSTOR_unpack</code>, followed by <code>JSTOR_corpusofnouns</code> to create a corpus to prepare the data for MALLET, and then <code>JSTOR_MALLET</code> to generate topic models using MALLET. Explore topic models with <code>JSTOR_MALLET_topicsovertime</code> and <code>JSTOR_MALLET_topicinfo</code>. See more about MALLET here <a href="http://mallet.cs.umass.edu/topics.php">http://mallet.cs.umass.edu/topics.php</a> and <a href="http://programminghistorian.org/lessons/topic-modeling-and-mallet">http://programminghistorian.org/lessons/topic-modeling-and-mallet</a> </p>

<h2>Limitations and Disclaimer</h2>

<p>Currently this package is intended for the exploration of a single journal archive. For example, all of the articles held by JSTOR of one journal or on one subject. It may be useful for other types of DfR archives, but has not yet been widely tested. Also, I am not a programmer, computer scientist, linguist, statistician, lawyer, etc. This software is provided as-is, in the hope that it may be useful, but without any warranty or support, whatsoever. This is a work in progress and there is currently very little custom error handling (the more cryptic errors are usually due to a search for a word or bigram that does not exist in the archive). Use at your own risk, and fork and share as you like. </p>

<h2>Acknowledgements</h2>

<p>Many of the best ideas for these functions have come directly from the prolific and creative research and coding of Andrew Goldstone, Jonathan Goodwin, Shawn Graham, Matt Jockers, David Mimno, Ben Schmidt and Ted Underwood. None of them are responsible for the consequences of use of this package, no matter how awful, even if they arise from flaws in it (of course I take full responsibility for the flaws). Magdalena Balazinska provided access to computing resources for development and testing, for which I&#39;m most grateful. Thanks to Ian Kretzler, Jiun-Yu Liu and Joss Whittaker for intensive testing and many useful suggestions.</p>

</body>

</html>

